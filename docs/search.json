[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LLaMPPL.clj",
    "section": "",
    "text": "1 Preface\nThis repo explores the LLaMPPL underlying algorithms from Clojure using llama.clj.\nSee Sequential Monte Carlo Steering of Large Language Models using Probabilistic Programs by Alexander K. Lew, Tan Zhi-Xuan, Gabriel Grand, Vikash K. Mansinghka.\nAt the moment, we demonstrate implementing the Sequential Monte Carlo algorithm on a specific case, the Hard Constraints case (See Figure 1 and Subection 2.2 of the paper). Specifically, we implement the “only short words” constraint using a certain choice of \\(M\\) (the Markove kernel) and \\(G\\) (the potential function) (but not the most efficient choice for that case).\nThe main effort so far has been in tackling some of the caching challenges.\nOur current bottom line is being able to generate a random population of texts (“particles” in SMC jargon) starting with “The Fed says” and using only words of at most 5 letters.\n\n(require '[llamppl.llm :as llm]\n         '[llamppl.smc :as smc]\n         '[tablecloth.api :as tc])\n\n\n(delay\n  (let [*smc-state (atom (smc/new-smc-state))]\n    (smc/run-smc! *smc-state\n                  {:cache-threshold 30\n                   :seed 1\n                   :base-text \"The Fed says\"\n                   :max-n-letters 5\n                   :N 10\n                   :K 3\n                   :initial-N 5\n                   :max-text-length 10})\n    (-&gt; @*smc-state\n        :particles\n        (tc/map-columns :finished [:x] llm/finished?)\n        (tc/map-columns :length [:x] count)\n        (tc/map-columns :text [:x] llm/untokenize)\n        (tc/drop-columns [:x])\n        (tc/set-dataset-name \"texts\")\n        (tech.v3.dataset.print/print-range :all))))\n\ntexts [10 6]:\n\n\n\n\n\n\n\n\n\n\n\n:w\n:time\n:gen\n:finished\n:length\n:text\n\n\n\n\n0.03333333\nThu Feb 15 01:36:35 IST 2024\n7\nfalse\n10\nThe Fed says it’s all about the money\n\n\n0.03333333\nThu Feb 15 01:36:35 IST 2024\n7\nfalse\n10\nThe Fed says it’s all about the money\n\n\n0.03333333\nThu Feb 15 01:36:37 IST 2024\n7\nfalse\n10\nThe Fed says it were but the GOP lies\n\n\n0.03333333\nThu Feb 15 01:36:37 IST 2024\n7\nfalse\n10\nThe Fed says it were but the GOP says\n\n\n0.03333333\nThu Feb 15 01:36:37 IST 2024\n7\nfalse\n10\nThe Fed says it were but the GOP won\n\n\n0.03333333\nThu Feb 15 01:36:37 IST 2024\n7\nfalse\n10\nThe Fed says it were but the GOP will\n\n\n0.03333333\nThu Feb 15 01:36:37 IST 2024\n7\nfalse\n10\nThe Fed says it were but the GOP says\n\n\n0.03333333\nThu Feb 15 01:36:37 IST 2024\n7\nfalse\n10\nThe Fed says it were but the rest of the\n\n\n0.03333333\nThu Feb 15 01:36:37 IST 2024\n7\nfalse\n10\nThe Fed says it were but the rest of the\n\n\n0.03333333\nThu Feb 15 01:36:37 IST 2024\n7\nfalse\n10\nThe Fed says it were but the rest of the\n\n\n\n\nsource: notebooks/index.clj",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "2  Setup",
    "section": "",
    "text": "(to be documented soon)\n\nsource: notebooks/setup.clj",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Setup</span>"
    ]
  },
  {
    "objectID": "llamppl.utils.html",
    "href": "llamppl.utils.html",
    "title": "3  A few utility functions",
    "section": "",
    "text": "3.1 Time\nGetting the time now (useful for reporting):\nFor example:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A few utility functions</span>"
    ]
  },
  {
    "objectID": "llamppl.utils.html#time",
    "href": "llamppl.utils.html#time",
    "title": "3  A few utility functions",
    "section": "",
    "text": "(defn now []\n  (java.util.Date.))\n\n\n\n(delay\n  (now))\n\n\n#inst \"2024-02-14T23:36:38.282-00:00\"",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A few utility functions</span>"
    ]
  },
  {
    "objectID": "llamppl.utils.html#math",
    "href": "llamppl.utils.html#math",
    "title": "3  A few utility functions",
    "section": "3.2 Math",
    "text": "3.2 Math\nNormalizing a vector or array of numbers so that their sum would be 1:\n\n(defn normalize [ws]\n  (fun// ws\n         (fun/sum ws)))\n\nFor example:\n\n(delay\n  (normalize [1 3 3 1]))\n\n\n[0.125 0.375 0.375 0.125]\n\n\nsource: notebooks/llamppl/utils.clj",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A few utility functions</span>"
    ]
  },
  {
    "objectID": "llamppl.llm.html",
    "href": "llamppl.llm.html",
    "title": "4  LLMs: Using llama.clj",
    "section": "",
    "text": "4.1 Constants\nPath to the LLM (assuming the MODELS_PATH environment variable is set properly):\nOne megabyte:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>LLMs: Using llama.clj</span>"
    ]
  },
  {
    "objectID": "llamppl.llm.html#constants",
    "href": "llamppl.llm.html#constants",
    "title": "4  LLMs: Using llama.clj",
    "section": "",
    "text": "(def llama7b-path\n  (str (System/getenv \"MODELS_PATH\")\n       \"/llama-2-7b-chat.ggmlv3.q4_0.bin\"))\n\n\n\n(def MB (math/pow 2 20))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>LLMs: Using llama.clj</span>"
    ]
  },
  {
    "objectID": "llamppl.llm.html#models",
    "href": "llamppl.llm.html#models",
    "title": "4  LLMs: Using llama.clj",
    "section": "4.2 Models",
    "text": "4.2 Models\nCreating a new model context:\n\n(defn new-llama-ctx []\n  (llama/create-context\n   llama7b-path\n   {:use-mlock true}))\n\nLet us keep one copy of an unchanged model (to extract basic information):\n\n(def base-llama-ctx\n  (new-llama-ctx))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>LLMs: Using llama.clj</span>"
    ]
  },
  {
    "objectID": "llamppl.llm.html#tokens",
    "href": "llamppl.llm.html#tokens",
    "title": "4  LLMs: Using llama.clj",
    "section": "4.3 Tokens",
    "text": "4.3 Tokens\nA function to turn a String of text to a list of tokens:\n\n(defn tokenize [text]\n  (llutil/tokenize base-llama-ctx text))\n\nExample:\n\n(delay\n  (-&gt; \"The Fed says\"\n      tokenize))\n\n\n[1576 17972 4083]\n\nA function to turn a list of tokens to a String of text:\n\n(defn untokenize [tokens]\n  (llutil/untokenize base-llama-ctx tokens))\n\nExample:\n\n(delay\n  (-&gt; \"The Fed says\"\n      tokenize\n      untokenize))\n\n\n\"The Fed says\"\n\nA map from tokens to the corresponding strings:\n\n(def token-&gt;str\n  (into (sorted-map)\n        (comp (map\n               (fn [token]\n                 [token (raw/llama_token_to_str base-llama-ctx token)]))\n              (take-while (fn [[token untoken]]\n                            untoken)))\n        (range 0 Integer/MAX_VALUE)))\n\nExample:\n\n(delay\n  (-&gt;&gt; \"The Fed says\"\n       tokenize\n       (map token-&gt;str)))\n\n\n(\"The\" \" Fed\" \" says\")\n\nThe EOS (end-of-sentence) token:\n\n(def llama-eos (llama/eos base-llama-ctx))\n\nChecking whether a sequence of tokens has ended.\n\n(defn finished? [tokens]\n  (-&gt;&gt; tokens\n       (some (partial = llama-eos))\n       some?))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>LLMs: Using llama.clj</span>"
    ]
  },
  {
    "objectID": "llamppl.llm.html#probabilities",
    "href": "llamppl.llm.html#probabilities",
    "title": "4  LLMs: Using llama.clj",
    "section": "4.4 Probabilities",
    "text": "4.4 Probabilities\nExample: Getting next-token logits for a given piece of text.\n\n(delay\n  (-&gt; (new-llama-ctx)\n      ;; Note thwe are **mutating** the context\n      (llama/llama-update \"How much wood would a\")\n      llama/get-logits\n      (-&gt;&gt; (take 5))\n      vec))\n\n\n[-4.3994875 -4.41101 3.215666 -3.3254027 -1.6136708]\n\nLet us look at the distribution of logits.\n\n(delay\n  (-&gt; (new-llama-ctx)\n      (llama/llama-update \"How much wood would a\")\n      llama/get-logits\n      (-&gt;&gt; (hash-map :logit))\n      tc/dataset\n      (hanami/histogram :logit {:nbins 100})\n      (assoc :height 200)))\n\n\nExample: Picking the next token of the highest probability.\n\n(delay\n  (let [llama-ctx (new-llama-ctx)]\n    (-&gt; llama-ctx\n        (llama/llama-update \"How much wood would a\")\n        llama/get-logits\n        argops/argmax\n        token-&gt;str)))\n\n\n\" wood\"",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>LLMs: Using llama.clj</span>"
    ]
  },
  {
    "objectID": "llamppl.llm.html#keeping-copies-of-context-state",
    "href": "llamppl.llm.html#keeping-copies-of-context-state",
    "title": "4  LLMs: Using llama.clj",
    "section": "4.5 Keeping copies of context state",
    "text": "4.5 Keeping copies of context state\n\n(def state-size\n  (-&gt; base-llama-ctx\n      (raw/llama_get_state_size)))\n\nHow big is this state?\n\n(delay\n  (-&gt; state-size\n      (/ MB)\n      (-&gt;&gt; (format \"%.02f MB\"))))\n\n\n\"258.18 MB\"\n\nLet us keep a copy of the state of our base context:\n\n(def base-state-data\n  (let [mem (byte-array state-size)]\n    (raw/llama_copy_state_data base-llama-ctx mem)\n    mem))\n\n\nsource: notebooks/llamppl/llm.clj",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>LLMs: Using llama.clj</span>"
    ]
  },
  {
    "objectID": "llamppl.cache.html",
    "href": "llamppl.cache.html",
    "title": "5  Caching model state",
    "section": "",
    "text": "5.1 A storage space\nLet us create a space to store a few model states. For now, the number of slots we use is hardcoded, fitting our JVM heap space.\nWe have 70 huge byte arrays:\nAs an example, let us try the following: * Use our model context to compute the next word for a piece text. * Store the model state. * Use our model with another text. * Retrieve the states we strored. * Check the next word again - is it the same as the one in the beginning?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Caching model state</span>"
    ]
  },
  {
    "objectID": "llamppl.cache.html#a-storage-space",
    "href": "llamppl.cache.html#a-storage-space",
    "title": "5  Caching model state",
    "section": "",
    "text": "(def n-states 70)\n\n\n(defonce states-storage\n  (vec (repeatedly\n        n-states\n        #(byte-array llm/state-size))))\n\n\n\n(delay\n  (-&gt;&gt; states-storage\n       (map count)\n       frequencies))\n\n\n{270726188 70}\n\n\n\n(delay\n  (let [llama-ctx (llm/new-llama-ctx)\n        get-next-word (fn [llama-ctx]\n                        (-&gt; llama-ctx\n                            llama/get-logits\n                            argops/argmax\n                            llm/token-&gt;str))\n        ;; Compute the word (recall that llama updates are mutating the context).\n        word-at-storage (-&gt; llama-ctx\n                            (llama/llama-update \"How much wood would a\")\n                            get-next-word)\n        ;; Store the model state\n        _ (-&gt; llama-ctx\n              (raw/llama_copy_state_data (states-storage 0)))\n        ;; Update the model with another text, getting other words.\n        another-word (-&gt; llama-ctx\n                         (llama/llama-update \"How are you\")\n                         get-next-word)\n        ;; Retrieve the model state we stored earlier.\n        _ (-&gt; llama-ctx\n              (raw/llama_set_state_data (states-storage 0)))\n        ;; Compute the next word again\n        word-after-retrieval (-&gt; llama-ctx\n                                 get-next-word)]\n    {:word-at-storage word-at-storage\n     :another-word another-word\n     :word-after-retrieval word-after-retrieval}))\n\n\n{:word-at-storage \" wood\",\n :another-word \"?\",\n :word-after-retrieval \" wood\"}",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Caching model state</span>"
    ]
  },
  {
    "objectID": "llamppl.cache.html#fifo-cache",
    "href": "llamppl.cache.html#fifo-cache",
    "title": "5  Caching model state",
    "section": "5.2 FIFO cache",
    "text": "5.2 FIFO cache\nWe will manage a basic FIFO cache on top of this storage.\nIts API tries to be similar to the clojure.cache API, with some differences.\nTODO: Document the cache API better.\n\n(defn new-fifo-cache []\n  {:id-&gt;idx {}\n   :idx-&gt;id {}\n   :current-idx 0})\n\n\n(defn lookup-or-miss!-impl [*fifo-cache id mem-cpy-fn]\n  (let [{:keys [id-&gt;idx]} @*fifo-cache]\n    (or (some-&gt; id\n                id-&gt;idx\n                states-storage)\n        (-&gt; *fifo-cache\n            (swap! (fn [fifo-cache]\n                     (let [updated-fifo-cache\n                           (as-&gt; fifo-cache fc\n                             (update fc :current-idx\n                                     (fn [idx] (-&gt; idx inc (rem n-states))))\n                             (update fc :id-&gt;idx dissoc ((:idx-&gt;id fc)\n                                                         (:current-idx fc)))\n                             (update fc :id-&gt;idx assoc id (:current-idx fc))\n                             (update fc :idx-&gt;id assoc (:current-idx fc) id))]\n                       (-&gt; updated-fifo-cache\n                           :current-idx\n                           states-storage\n                           mem-cpy-fn)\n                       updated-fifo-cache)))\n            :current-idx\n            states-storage))))\n\n\n(def lookup-or-miss!\n  (let [*id (atom 0)]\n    (fn [{:keys [state-id\n                 llama-ctx-fn\n                 *cache]}]\n      (let [id (or state-id (swap! *id inc))]\n        {:state-id id\n         :state-data (lookup-or-miss!-impl\n                      *cache\n                      id\n                      (fn [mem]\n                        (raw/llama_copy_state_data\n                         (llama-ctx-fn)\n                         mem)))}))))\n\n\n(defn has? [*fifo-cache id]\n  (-&gt; @*fifo-cache\n      :id-&gt;idx\n      (contains? id)))\n\n\n(defn lookup [*fifo-cache id]\n  (-&gt; @*fifo-cache\n      :id-&gt;idx\n      (get id)\n      states-storage))\n\nAs an example, let us try using this cache with a scenario similar to the one we tried earlier.\n\n(delay\n  (let [llama-ctx (llm/new-llama-ctx)\n        get-next-word (fn [llama-ctx]\n                        (-&gt; llama-ctx\n                            llama/get-logits\n                            argops/argmax\n                            llm/token-&gt;str))\n        *cache (atom (new-fifo-cache))\n        ;; Use the cache a bit, storing a few states.\n        _ (dotimes [i 3]\n            (lookup-or-miss!\n             {:*cache *cache\n              :llama-ctx-fn #(llama/llama-update\n                              llama-ctx\n                              \"How are you\")}))\n        ;; Use the cache for a text we care about,\n        ;; keeping the `state-id` for future reference.\n        ;; We also keep the cached `state-data` itself,\n        ;; for our testing.\n        {:keys [state-id\n                state-data]} (lookup-or-miss!\n                              {:*cache *cache\n                               :llama-ctx-fn #(llama/llama-update\n                                               llama-ctx\n                                               \"How much wood would a\")})\n        ;; Compute the next word for the text we used.\n        word-at-storage (get-next-word llama-ctx)\n        ;; Keep updating the model state and using the cache.\n        _ (dotimes [i 3]\n            (lookup-or-miss!\n             {:*cache *cache\n              :llama-ctx-fn #(llama/llama-update\n                              llama-ctx\n                              \"How are you\")}))\n        ;; Compute the next word, which should be another word.\n        another-word (get-next-word llama-ctx)\n        ;; Retrieve the model state from the cache\n        ;; using the `state-id` we remembered.\n        retrieved-state-data (lookup *cache state-id)\n        ;; Used the retrived state for the model state:\n        _ (-&gt; llama-ctx\n              (raw/llama_set_state_data\n               retrieved-state-data))\n        ;; Compate the retrieved state with the state we kept earlier.\n        states-comparison (java.util.Arrays/equals\n                           ^bytes state-data\n                           ^bytes retrieved-state-data)\n        ;; Copmute the next word again,\n        ;; expecting to get the same the same one we got earlier.\n        word-after-retrieval (get-next-word llama-ctx)]\n    {:word-at-storage word-at-storage\n     :another-word another-word\n     :states-comparison states-comparison\n     :word-after-retrieval word-after-retrieval}))\n\n\n{:word-at-storage \" wood\",\n :another-word \"How\",\n :states-comparison true,\n :word-after-retrieval \" wood\"}\n\n\nsource: notebooks/llamppl/cache.clj",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Caching model state</span>"
    ]
  },
  {
    "objectID": "llamppl.trie.html",
    "href": "llamppl.trie.html",
    "title": "6  A token-trie cache",
    "section": "",
    "text": "6.1 Evaluation\nWe define cached evaluation as a recursive transformation of a context map. Note that it is not a pure functional transformation, as some parts of the map, spefifically the llama-ctx model context and the *cache atom, are mutable.\nHere is how we create a context map to be passed to the evaluation:\nGiven an atom holding a context map, we may update it with a given cached evaluation. Note that we do not keep all the fields of the context map. Some of them (e.g., :sub-trie) were used for the recursive evaluation and need to be discarded (using select-keys) to avoid affecting the next evaluation.\nTypically, we not only update the model with a sequence of tokens, but are also interested in the model logits for the next token.\nFor example, let us pick the tokens with highest logits (that is, the most probable tokens) following a few texts:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>A token-trie cache</span>"
    ]
  },
  {
    "objectID": "llamppl.trie.html#evaluation",
    "href": "llamppl.trie.html#evaluation",
    "title": "6  A token-trie cache",
    "section": "",
    "text": "(defn cached-eval [{:as context\n                    :keys [llama-ctx\n                           llama-ctx-state-id\n                           trie\n                           *cache\n                           tokens\n                           path\n                           sub-trie\n                           remaining-tokens]\n                    :or {sub-trie trie\n                         path []\n                         remaining-tokens tokens}}]\n  (let [path-&gt;text (fn [path]\n                     (-&gt;&gt; path\n                          (filter number?)\n                          llm/untokenize))]\n    (if (empty? remaining-tokens)\n      ;; done - return this context\n      (do\n        #_(prn [:done (path-&gt;text path)])\n        context)\n      ;; else\n      (let [token (first remaining-tokens)\n            ;; Look into the next sub trie,\n            ;; following this token:\n            next-step [:children token]\n            next-path (concat path next-step)\n            next-sub-trie (get-in sub-trie next-step)]\n        (if (some-&gt;&gt; next-sub-trie\n                     :llama-state-id\n                     (cache/has? *cache))\n          ;; We have already created next-sub-trie in the past,\n          ;; and we have its llama state still in the cache,\n          ;; so let us step into it.\n          (do\n            #_(prn [:recur-known (path-&gt;text next-path)])\n            (recur (-&gt; context\n                       (assoc\n                        :sub-trie next-sub-trie\n                        :path next-path\n                        :remaining-tokens (rest remaining-tokens)\n                        :logits (:logits next-sub-trie)))))\n          ;; Else, we need to create the next sub trie.\n          (let [{:keys [state-id state-data]}\n                (cache/lookup-or-miss!\n                 {:*cache *cache\n                  :llama-ctx-fn (fn []\n                                  ;; Make sure the llama-ctx has the right state\n                                  ;; to continue.\n                                  (cond\n                                    ;; When we are in the beginning of the path,\n                                    ;; take the base state.\n                                    (= path [])\n                                    (do\n                                      #_(prn [:set-from-base])\n                                      (raw/llama_set_state_data llama-ctx\n                                                                llm/base-state-data))\n                                    ;; When the last evaluation does not fit\n                                    ;; out place in the trie,\n                                    ;; bring the reletant state from cache.\n                                    (-&gt; sub-trie\n                                        :llama-state-id\n                                        (not= llama-ctx-state-id))\n                                    (do\n                                      #_(prn [:set-from-cache])\n                                      (-&gt;&gt; sub-trie\n                                           :llama-state-id\n                                           (cache/lookup *cache)\n                                           (raw/llama_set_state_data llama-ctx)))\n                                    ;; Otherwise, our current state is what we need.\n                                    :else\n                                    (do #_(prn [:continue])\n                                        nil))\n                                  ;; Evaluate the current token:\n                                  (prn [:eval\n                                        (path-&gt;text path)\n                                        '--&gt;\n                                        (llm/token-&gt;str token)])\n                                  (time\n                                   (llama/llama-update llama-ctx\n                                                       token\n                                                       ;; n-past\n                                                       (-&gt;&gt; path\n                                                            (filter number?)\n                                                            count)\n                                                       ;; num-threads\n                                                       8))\n                                  #_(prn [:extract-state])\n                                  llama-ctx)})\n                ;; Create the next sub trie:\n                new-sub-trie (merge next-sub-trie\n                                    {:logits (llama/get-logits llama-ctx)\n                                     :llama-state-id state-id})]\n            ;; Step into the next sub trie:\n            (do #_(prn [:recur-new (path-&gt;text next-path)])\n                (recur (-&gt; context\n                           (update :trie assoc-in next-path new-sub-trie)\n                           (assoc :llama-ctx-state-id state-id\n                                  :sub-trie new-sub-trie\n                                  :path next-path\n                                  :remaining-tokens (rest remaining-tokens)\n                                  :logits (:logits new-sub-trie)))))))))))\n\n\n\n(defn new-context\n  ([]\n   (new-context {}))\n  ([{:keys [seed]\n     :or {seed 12345}}]\n   (System/gc)\n   (let [llama-ctx (llm/new-llama-ctx)\n         samplef (llama/init-mirostat-v2-sampler\n                  llama-ctx)]\n     (prn [:seed seed])\n     (raw/llama_set_rng_seed llama-ctx seed)\n     {:llama-ctx llama-ctx\n      :samplef samplef\n      :trie {}\n      :*cache (atom (cache/new-fifo-cache))})))\n\n\n\n(defn cached-eval! [*context tokens]\n  (let [context (-&gt; @*context\n                    (assoc :tokens tokens)\n                    cached-eval)]\n    (reset! *context\n            (select-keys context [:llama-ctx :*cache :samplef :trie :logits]))\n    context))\n\n\n\n(defn logits! [*context tokens]\n  (-&gt; *context\n      (cached-eval! tokens)\n      :logits))\n\n\n\n(delay\n  (let [*context (atom (new-context))]\n    (-&gt;&gt; [\"How are\"\n          \"How much wood would a\"\n          \"How much wood could a\"]\n         (mapv (fn [text]\n                 [text '--&gt; (-&gt;&gt; text\n                                 llm/tokenize\n                                 (logits! *context)\n                                 argops/argmax\n                                 llm/token-&gt;str)]))\n         kind/fragment)))\n\n\n[\"How are\" --&gt; \" you\"]\n\n\n[\"How much wood would a\" --&gt; \" wood\"]\n\n\n[\"How much wood could a\" --&gt; \" wood\"]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>A token-trie cache</span>"
    ]
  },
  {
    "objectID": "llamppl.trie.html#visualising-the-trie",
    "href": "llamppl.trie.html#visualising-the-trie",
    "title": "6  A token-trie cache",
    "section": "6.2 Visualising the trie",
    "text": "6.2 Visualising the trie\n\n(defn visualize-trie [context]\n  (let [{:keys [*cache trie]} context\n        *node-id (atom 0)\n        *nodes (atom [{:data {:id \"0\" :word \"(root)\"}}])\n        *edges (atom [])\n        trie (-&gt; trie\n                 (assoc :node-id (str @*node-id))\n                 (-&gt;&gt; (walk/prewalk\n                       (fn [v]\n                         (if (:children v)\n                           (-&gt; v\n                               (update\n                                :children\n                                (fn [children]\n                                  (-&gt;&gt; children\n                                       (map\n                                        (fn [[token child]]\n                                          (let [node-id (str (swap! *node-id inc))]\n                                            (swap!\n                                             *nodes\n                                             conj\n                                             {:data {:id node-id\n                                                     :token token\n                                                     :word (llm/token-&gt;str token)\n                                                     :background (if (-&gt;&gt; child\n                                                                          :llama-state-id\n                                                                          (cache/has? *cache))\n                                                                   \"lightgreen\"\n                                                                   \"lightgrey\")}})\n                                            [token (-&gt; child\n                                                       (assoc :node-id node-id))])))\n                                       (into {})))))\n                           v)))\n                      (walk/prewalk (fn [v]\n                                      (if (:logits v)\n                                        (dissoc v :logits)\n                                        v)))\n                      (walk/prewalk\n                       (fn [v]\n                         (if-let [{:keys [node-id]} v]\n                           (do\n                             (-&gt;&gt; v\n                                  :children\n                                  vals\n                                  (map\n                                   (fn [child]\n                                     (let [child-node-id (:node-id child)]\n                                       {:data {:id (str node-id \"-\" child-node-id)\n                                               :source node-id\n                                               :target child-node-id}})))\n                                  (swap! *edges concat))\n                             v)\n                           v)))))]\n    (kind/cytoscape\n     {;:trie trie\n      :elements {:nodes @*nodes\n                 :edges @*edges}\n      :style [{:selector \"node\"\n               :css {:content \"data(word)\"\n                     :text-valign \"center\"\n                     :text-halign \"center\"\n                     :height 50\n                     :width 50\n                     :background-color \"data(background)\"}}\n              {:selector \"edge\"\n               :css {:curve-style \"bezier\"\n                     :target-arrow-shape \"triangle\"}}]\n      :layout {:name \"cose\"}})))\n\nFor example, let us run a few “next token” tasks as above, but this time, also visualize the reulting trie.\n\n(delay\n  (let [*context (atom (new-context))]\n    (kind/fragment\n     [(-&gt;&gt; [\"How are\"\n            \"How much wood would a\"\n            \"How much wood could a\"]\n           (mapv (fn [text]\n                   [text '--&gt; (-&gt;&gt; text\n                                   llm/tokenize\n                                   (logits! *context)\n                                   argops/argmax\n                                   llm/token-&gt;str)])))\n      (visualize-trie @*context)])))\n\n\n[[\"How are\" --&gt; \" you\"]\n [\"How much wood would a\" --&gt; \" wood\"]\n [\"How much wood could a\" --&gt; \" wood\"]]\n\n\nHere is a bigger example. Note that some nodes are no longer in the cache and are thus coloured differently. (Source)\n\n(delay\n  (let [*context (atom (new-context))]\n    (-&gt;&gt; [\"The groundhog (Marmota monax), also known as the woodchuck, is a rodent of the family Sciuridae, belonging to the group of large ground squirrels known as marmots. The groundhog is a lowland creature of North America; it is found through much of the Eastern United States, across Canada and into Alaska. It was first scientifically described by Carl Linnaeus in 1758.\"\n          \"The groundhog is also referred to as a chuck, wood-shock, groundpig, whistlepig, whistler, thickwood badger, Canada marmot, monax, moonack, weenusk, red monk, land beaver, and, among French Canadians in eastern Canada, siffleux. The name \\\"thickwood badger\\\" was given in the Northwest to distinguish the animal from the prairie badger. Monax (Móonack) is an Algonquian name of the woodchuck, which means \\\"digger\\\" (cf. Lenape monachgeu). Young groundhogs may be called chucklings. \"\n          \"The groundhog did visit me yesterday.\"\n          \"The groundhog is also referred to as Margaret. At least that is how they call her in our neighbourhood.\"]\n         (run! (fn [text]\n                 (-&gt;&gt; text\n                      llm/tokenize\n                      (logits! *context)\n                      argops/argmax\n                      llm/token-&gt;str))))\n    (visualize-trie @*context)))",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>A token-trie cache</span>"
    ]
  },
  {
    "objectID": "llamppl.trie.html#sampling-random-tokens",
    "href": "llamppl.trie.html#sampling-random-tokens",
    "title": "6  A token-trie cache",
    "section": "6.3 Sampling random tokens",
    "text": "6.3 Sampling random tokens\nOur context holds a sample function samplef, which allows us to sample tokens according to logits.\nFor example:\n\n(delay\n  (let [*context (atom (new-context {:seed 1}))\n        {:keys [samplef]} @*context]\n    (-&gt;&gt; [\"How much wood would a\"\n          \"How much wood would a woodchuck\"\n          \"How much wood would a woodchuck chuck\"]\n         (mapv\n          (fn [text]\n            (let [logits (-&gt;&gt; text\n                              llm/tokenize\n                              (logits! *context))]\n              [text\n               (-&gt;&gt; (repeatedly\n                     1000\n                     #(llm/token-&gt;str (samplef logits)))\n                    frequencies)]))))))\n\n\n[[\"How much wood would a\" {\" wood\" 949, \" Wood\" 51}]\n [\"How much wood would a woodchuck\" {\" ch\" 997, \"...\" 3}]\n [\"How much wood would a woodchuck chuck\" {\" if\" 919, \",\" 79, \"\\n\" 2}]]\n\nFor convenience, let us use this function to sample one token. Note that we change the seed for diversity.\nTODO: Handle seeds more carefully for reproducibility..\n\n(defn sample-once! [*context logits]\n  (-&gt; @*context\n      :llama-ctx\n      (raw/llama_set_rng_seed (rand-int 9999)))\n  ((:samplef @*context)\n   logits))\n\nFor example:\n\n(delay\n  (let [*context (atom (new-context))]\n    (-&gt;&gt; \"How much wood would a\"\n         llm/tokenize\n         (logits! *context)\n         (sample-once! *context)\n         llm/token-&gt;str)))\n\n\n\" wood\"\n\n\nsource: notebooks/llamppl/trie.clj",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>A token-trie cache</span>"
    ]
  },
  {
    "objectID": "llamppl.smc.html",
    "href": "llamppl.smc.html",
    "title": "7  SMC Sampling of a probabilistic model",
    "section": "",
    "text": "7.1 A probabilistic model\nThe LLaMPPL paper defines a probabilistic model using an initial state \\(s_0\\), a Markove kernel \\(M\\), and a potential function \\(G\\).\nHere we implmenti a specific model of the ‘hard constraints’ type: generating texts that use only short words.\nNote that the paper offers more than one option for the Markov kernel \\(M\\) and the potential function \\(G\\) for that case. For now, we are not using the most efficient choice of them.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>SMC Sampling of a probabilistic model</span>"
    ]
  },
  {
    "objectID": "llamppl.smc.html#a-probabilistic-model",
    "href": "llamppl.smc.html#a-probabilistic-model",
    "title": "7  SMC Sampling of a probabilistic model",
    "section": "",
    "text": "7.1.1 The Markov kernel\nWe define \\(M\\) as a sampling step (which is the way we use it algorithmically).\n\n(defn M-step [*context\n              previous-tokens]\n  (if (llm/finished? previous-tokens)\n    previous-tokens\n    (-&gt;&gt; previous-tokens\n         (trie/logits! *context)\n         (trie/sample-once! *context)\n         (conj previous-tokens))))\n\nFor example:\n\n(delay\n  (let [*context (atom (trie/new-context {:seed 1}))]\n    (kind/fragment\n     [(-&gt;&gt; #(-&gt;&gt; \"How much wood\"\n                 llm/tokenize\n                 (iterate (partial M-step *context))\n                 (take 10)\n                 last\n                 llm/untokenize)\n           (repeatedly 5)\n           vec)\n      (trie/visualize-trie @*context)])))\n\n\n[\"How much wood would a woodchuck chuck if a\"\n \"How much wood a woodchuck can chuck?” The\"\n \"How much wood\\n Hinweis: Hier ist die Menge des\"\n \"How much wood desires there are, wood it chooses\"\n \"How much woodoses does a woodchuck have?”\\n\"]\n\n\n\n\n7.1.2 The potential function\nIn our current implementation, the potential function \\(G\\) is a simple representation of our constraint: requiring only short words. The maximal number of letters is a parameter, max-n-letters.\n\n(defn G [max-n-letters current-tokens]\n  (if (-&gt; current-tokens\n          llm/untokenize\n          (str/split  #\" \")\n          (-&gt;&gt; (every? #(-&gt; % count (&lt;= max-n-letters)))))\n    1 0))\n\nFor example, let us create a random sequence of tokens and check whether it satisfies \\(G\\) with different values of max-n-letters.\n\n(delay\n  (let [*context (atom (trie/new-context {:seed 1}))\n        tokens (-&gt;&gt; \"How much wood\"\n                    llm/tokenize\n                    (iterate (partial M-step *context))\n                    (take 10)\n                    last)]\n    {:text (llm/untokenize tokens)\n     :G5 (G 5 tokens)\n     :G9 (G 9 tokens)}))\n\n\n{:text \"How much wood would a woodchuck chuck if a\", :G5 0, :G9 1}",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>SMC Sampling of a probabilistic model</span>"
    ]
  },
  {
    "objectID": "llamppl.smc.html#smc-implementation",
    "href": "llamppl.smc.html#smc-implementation",
    "title": "7  SMC Sampling of a probabilistic model",
    "section": "7.2 SMC implementation",
    "text": "7.2 SMC implementation\nHere we implement the Sequential Monte Carlo Transformer Steering algorithm, Algorithm 1 of the paper.\nTODO: Explain this part better.\nAn auxiliary function to find \\(c*\\) (see the algorithm):\n\n(defn find-c [weights N]\n  (prn [:weights weights\n        :N N])\n  (let [sorted-weights (vec (sort weights))]\n    (loop [B-val 0.0\n           A-val (count weights)\n           i 0]\n      (let [chi (sorted-weights i)\n            new-A-val (dec A-val)\n            new-B-val (+ B-val chi)]\n        (if (= i N)\n          N\n          (if (-&gt; new-B-val\n                  (/ chi)\n                  (+ new-A-val)\n                  (- N)\n                  (&lt;= 1e-12))\n            (/ (- N new-A-val)\n               new-B-val)\n            (recur new-B-val\n                   new-A-val\n                   (inc i))))))))\n\nFor example:\n\n(delay\n  (find-c [0.1 0.2] 10))\n\n\n90.0\n\nThe SMC loop will manage a stateful atom *smc-state. This allows us to conveniently inspect the process from another thread while it is running.\n\n(defn new-smc-state [] {:stop false\n                        :particles []})\n\n\n(defn run-smc!\n  [*smc-state\n   {:keys [cache-threshold\n           seed\n           max-n-letters\n           N\n           K\n           base-text\n           initial-N\n           max-text-length]}]\n  (let [*context (atom (trie/new-context {:seed 1}))\n        s0 (llm/tokenize base-text)]\n    (swap! *smc-state\n           assoc :particles  (tc/dataset {:x (repeat initial-N s0)\n                                          :w 1\n                                          :time (repeat initial-N (utils/now))\n                                          :gen 0}))\n    (loop [gen 1]\n      (let [particles (:particles @*smc-state)\n            finished (-&gt;&gt; particles\n                          :x\n                          (map llm/finished?))\n            done (fun/or finished\n                         (-&gt;&gt; particles\n                              :x\n                              (map #(-&gt; % count (&gt;= max-text-length)))))]\n        (-&gt;&gt; finished\n             frequencies\n             (vector :finished-freqs)\n             prn)\n        (if (or (:stop @*smc-state)\n                (every? true? done))\n          {:particles particles\n           :Z (-&gt; particles :w fun/mean)}\n          ;; else\n          (let [K (-&gt;&gt; done\n                       (map (fn [f]\n                              (if f 1 K))))\n                N-prime (fun/sum K)\n                new-particles (-&gt; particles\n                                  (tc/add-columns {:K K\n                                                   :done done})\n                                  (tc/rows :as-maps)\n                                  (-&gt;&gt; (map\n                                        (fn [{:keys [x w done K]\n                                              :as row}]\n                                          (if done\n                                            (tc/dataset {:x [x]\n                                                         :w [(* w N-prime (/ N))]\n                                                         :time [(:time row)]\n                                                         :gen [(:gen row)]})\n                                            ;; else\n                                            (-&gt; (range K)\n                                                (-&gt;&gt; (map (fn [k]\n                                                            (-&gt; {:x (M-step *context x)\n                                                                 :time (utils/now)\n                                                                 :gen gen}))))\n                                                tc/dataset\n                                                (tc/map-columns\n                                                 :w\n                                                 [:x]\n                                                 (fn [x]\n                                                   (* (/ N-prime\n                                                         (* K N))\n                                                      w\n                                                      (G max-n-letters x))))))))\n                                       (apply tc/concat))\n                                  (tc/add-column :w #(-&gt; % :w utils/normalize))\n                                  ((fn [{:keys [x w time gen]\n                                         :as new-particles}]\n                                     (prn [:new-particles new-particles])\n                                     (let [w-sum (fun/sum w)\n                                           c* (find-c w N)\n                                           indexes (-&gt; new-particles\n                                                       tc/row-count\n                                                       range)\n                                           I-det (-&gt;&gt; indexes\n                                                      (filter (fn [i]\n                                                                (-&gt; i\n                                                                    w\n                                                                    (* c*)\n                                                                    (&gt;= 1)))))\n                                           I-stoch (-&gt;&gt; indexes\n                                                        (filter (fn [i]\n                                                                  (-&gt; i\n                                                                      w\n                                                                      (* c*)\n                                                                      (&lt; 1))))\n                                                        vec)\n                                           alpha (/ (-&gt;&gt; I-stoch\n                                                         (map w)\n                                                         fun/sum)\n                                                    (- N (count I-det)))\n                                           I-strat (loop [candidates I-stoch\n                                                          U (* alpha (rand))\n                                                          I-strat []]\n                                                     (if (empty? candidates)\n                                                       I-strat\n                                                       (let [i (first candidates)\n                                                             U (- U (w i))]\n                                                         (if (neg? U)\n                                                           (recur (rest candidates)\n                                                                  (+ U alpha)\n                                                                  (conj I-strat i))\n                                                           (recur (rest candidates)\n                                                                  U\n                                                                  I-strat)))))]\n                                       (prn [:c* c*\n                                             :I-det I-det\n                                             :I-stoch I-stoch\n                                             :I-strat I-strat])\n                                       (tc/dataset\n                                        (concat (-&gt;&gt; I-det\n                                                     (map (fn [i]\n                                                            {:x (x i)\n                                                             :w (* (w i)\n                                                                   (/ N N-prime))\n                                                             :time (time i)\n                                                             :gen (gen i)})))\n                                                (-&gt;&gt; I-strat\n                                                     (map (fn [i]\n                                                            {:x (x i)\n                                                             :w (* (/ N N-prime c*)\n                                                                   w-sum)\n                                                             :time (time i)\n                                                             :gen (gen i)})))))))))]\n            (swap! *smc-state\n                   assoc :particles new-particles)\n            (recur (inc gen))))))))\n\nLet us run an example: funding random continuations of the prefix “The Fed say” using only short words (5 letters most).\n\n(delay\n  (let [*smc-state (atom (new-smc-state))]\n    (run-smc! *smc-state\n              {:cache-threshold 30\n               :seed 1\n               :base-text \"The Fed says\"\n               :max-n-letters 5\n               :N 10\n               :K 3\n               :initial-N 5\n               :max-text-length 15})\n    (-&gt; @*smc-state\n        :particles\n        (tc/map-columns :finished [:x] llm/finished?)\n        (tc/map-columns :length [:x] count)\n        (tc/map-columns :text [:x] llm/untokenize)\n        (tc/drop-columns [:x])\n        (tc/set-dataset-name \"texts\")\n        (tech.v3.dataset.print/print-range :all)\n        kind/table)))\n\n\n\n\n:w\n:time\n:gen\n:finished\n:length\n:text\n\n\n\n\n0.03333333333333333\nThu Feb 15 01:40:25 IST 2024\n12\nfalse\n15\nThe Fed says “no” to a rate cut amid signs of a slow\n\n\n0.03333333333333333\nThu Feb 15 01:40:25 IST 2024\n12\nfalse\n15\nThe Fed says “no” to a rate cut amid signs of a weak\n\n\n0.03333333333333333\nThu Feb 15 01:40:26 IST 2024\n12\nfalse\n15\nThe Fed says “no” to a rate cut after the FOMC\n\n\n0.03333333333333333\nThu Feb 15 01:40:26 IST 2024\n12\nfalse\n15\nThe Fed says “no” to a rate cut after a more than two\n\n\n0.03333333333333333\nThu Feb 15 01:40:27 IST 2024\n12\nfalse\n15\nThe Fed says that the CAFR will be based on the 2\n\n\n0.03333333333333333\nThu Feb 15 01:40:27 IST 2024\n12\nfalse\n15\nThe Fed says that the CAFR will be based on the 2\n\n\n0.03333333333333333\nThu Feb 15 01:40:28 IST 2024\n12\nfalse\n15\nThe Fed says that the CAFR audit must be done by an\n\n\n0.03333333333333333\nThu Feb 15 01:40:29 IST 2024\n12\nfalse\n15\nThe Fed says that the CAFR of each bank is $3 tr\n\n\n0.03333333333333333\nThu Feb 15 01:40:29 IST 2024\n12\nfalse\n15\nThe Fed says that the CAFR of each bank is $33\n\n\n0.03333333333333333\nThu Feb 15 01:40:30 IST 2024\n12\nfalse\n15\nThe Fed says that the CAFR of each bank is $10\n\n\n\n\nsource: notebooks/llamppl/smc.clj",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>SMC Sampling of a probabilistic model</span>"
    ]
  }
]